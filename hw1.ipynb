{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c27574",
   "metadata": {},
   "source": [
    "### Exercise 1)\n",
    "L1:\n",
    "\n",
    "$L=\\dfrac{1}{2}(Y-y)^2+\\lambda\\sum_{i} |W_i|$\n",
    "\n",
    "L2:\n",
    "\n",
    "$L=\\dfrac{1}{2}(Y-y)^2+\\lambda\\sum_{i} W_i^2$\n",
    "\n",
    "L1 is more strict and it brings more weights to zero, thus making the weights sparse.\n",
    "\n",
    "L2 encourages smaller and more distributed weights across all features, reducing the impact of any individual feature. L2 doesn't make the weights exactly zero, but rather very small and close to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731f01b",
   "metadata": {},
   "source": [
    "### Exercise 2)\n",
    "In this part, you will implement a simple multi-layer perceptron neural network using PyTorch\n",
    "to solve a clothing classification problem. You have to work with the Fashion MNIST dataset,\n",
    "which consists of 10 classes with 60,000 examples in the training set and 10,000 examples in\n",
    "the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fda00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2a628",
   "metadata": {},
   "source": [
    "### data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb18f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations with mean of 0.5 and st of 0.5\n",
    "transform = transforms.Compose([  transforms.ToTensor(),  transforms.Normalize((0.5,), (0.5,)) ])\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "train_dataset = datasets.FashionMNIST( root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST( root='./data',train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314997a1",
   "metadata": {},
   "source": [
    "### model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72ce668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        # nn.ModuleList is used to contain multiple instances of PyTorch's nn.Module subclasses,\n",
    "        # such as linear layers (nn.Linear), activation functions (nn.ReLU, nn.Sigmoid, etc.), or other custom modules.\n",
    "        self.layers = nn.ModuleList()\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if i != len(sizes) - 2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "hidden_sizes = [256]  # Vary the hidden layer sizes here\n",
    "\n",
    "model = MLP(input_size, output_size, hidden_sizes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb73c1",
   "metadata": {},
   "source": [
    "### training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a8b538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [938/938], Loss: 0.0433\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0318\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0282\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0345\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0302\n",
      "Epoch [6/10], Step [938/938], Loss: 0.0212\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0306\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0312\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0306\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0252\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        # The line torch.eye(10)[targets] performs one-hot encoding on the targets by creating \n",
    "        # a one-hot encoded tensor of shape (batch_size, num_classes).\n",
    "        loss = criterion(outputs, torch.eye(10)[targets])\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bbbef",
   "metadata": {},
   "source": [
    "### evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e27e82f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.19%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "# torch.noused to disable gradient calculation. This is done to speed up the evaluation process and reduce\n",
    "# memory consumption since gradients are not needed during evaluation.\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        #  find the predicted class labels by selecting the class with the highest score from the outputs tensor. \n",
    "        # The torch.max function returns both the maximum values and their corresponding indices\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc7b1b",
   "metadata": {},
   "source": [
    "We did the experment with 1 hidden layer and achieved an accuracy of 81.65 percent. Now, we are going to increase the layer count to study the depth effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a36164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Step [938/938], Loss: 0.0542\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0530\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0418\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0517\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0446\n",
      "Epoch [6/10], Step [938/938], Loss: 0.0342\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0387\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0306\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0283\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0236\n",
      "Test Accuracy: 80.53%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "hidden_sizes = [256, 256]  # Vary the hidden layer sizes here\n",
    "\n",
    "model = MLP(input_size, output_size, hidden_sizes)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        # The line torch.eye(10)[targets] performs one-hot encoding on the targets by creating \n",
    "        # a one-hot encoded tensor of shape (batch_size, num_classes).\n",
    "        loss = criterion(outputs, torch.eye(10)[targets])\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "# torch.noused to disable gradient calculation. This is done to speed up the evaluation process and reduce\n",
    "# memory consumption since gradients are not needed during evaluation.\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        #  find the predicted class labels by selecting the class with the highest score from the outputs tensor. \n",
    "        # The torch.max function returns both the maximum values and their corresponding indices\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d331ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Step [938/938], Loss: 0.0664\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0527\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0542\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0482\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0382\n",
      "Epoch [6/10], Step [938/938], Loss: 0.0367\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0381\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0406\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0313\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0234\n",
      "Test Accuracy: 78.92%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "hidden_sizes = [256, 256, 256]  # Vary the hidden layer sizes here\n",
    "\n",
    "model = MLP(input_size, output_size, hidden_sizes)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        # The line torch.eye(10)[targets] performs one-hot encoding on the targets by creating \n",
    "        # a one-hot encoded tensor of shape (batch_size, num_classes).\n",
    "        loss = criterion(outputs, torch.eye(10)[targets])\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "# torch.noused to disable gradient calculation. This is done to speed up the evaluation process and reduce\n",
    "# memory consumption since gradients are not needed during evaluation.\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        #  find the predicted class labels by selecting the class with the highest score from the outputs tensor. \n",
    "        # The torch.max function returns both the maximum values and their corresponding indices\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bb357",
   "metadata": {},
   "source": [
    "We can conclude that the more we add hidden layers and increase the depth, the less accurate our model gets. This can be due to vanishing/exploding gradient or just the fact that we have more weights and they produce more error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231a0cec",
   "metadata": {},
   "source": [
    "### dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddf89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_d(nn.Module):\n",
    "    def __init__(self, sizes, dropout_rate):\n",
    "        super(MLP_d, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if i != len(sizes) - 2:\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(p=dropout_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957e3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_d(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Step [938/938], Loss: 0.0561\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0451\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0383\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0431\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0546\n",
      "Epoch [6/10], Step [938/938], Loss: 0.0424\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0436\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0349\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0275\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0339\n",
      "Test Accuracy: 80.98%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "sizes = [28 * 28, 256, 10]  \n",
    "\n",
    "model = MLP_d(sizes = sizes, dropout_rate = 0.4)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        # The line torch.eye(10)[targets] performs one-hot encoding on the targets by creating \n",
    "        # a one-hot encoded tensor of shape (batch_size, num_classes).\n",
    "        loss = criterion(outputs, torch.eye(10)[targets])\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "# torch.noused to disable gradient calculation. This is done to speed up the evaluation process and reduce\n",
    "# memory consumption since gradients are not needed during evaluation.\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in test_loader:\n",
    "        outputs = model(data)\n",
    "        #  find the predicted class labels by selecting the class with the highest score from the outputs tensor. \n",
    "        # The torch.max function returns both the maximum values and their corresponding indices\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca353c2",
   "metadata": {},
   "source": [
    "### early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "early_stopping_patience = 10\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, torch.eye(10)[labels])\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for val_inputs, val_labels in validation_loader:\n",
    "            val_inputs = val_inputs.view(-1, 28*28)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels.float()).item()\n",
    "        val_loss /= len(validation_loader)\n",
    "\n",
    "        # Check if validation loss has improved\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Check if early stopping criteria is met\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered! No improvement in {early_stopping_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
